{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 619
    },
    "id": "ocngcfKU8Lew",
    "outputId": "7c06da5c-1e52-4613-f94b-dab4c2f3ed62"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"mldatasetfinal numbers.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Assuming your dataset has features and no class labels\n",
    "features = data.drop(columns=[\"Price\"])  # Adjust the column name if needed\n",
    "\n",
    "# Handle missing values by imputing with mean\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "features_imputed = imputer.fit_transform(features)\n",
    "\n",
    "# Apply k-means with k = 3 (or 5 based on your dataset)\n",
    "k = 3\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "clusters = kmeans.fit_predict(features_imputed)\n",
    "\n",
    "# Apply PCA for visualization\n",
    "pca = PCA(n_components=2)\n",
    "features_pca = pca.fit_transform(features_imputed)\n",
    "\n",
    "# Create a DataFrame with the PCA components and cluster assignments\n",
    "df_pca = pd.DataFrame(features_pca, columns=[\"PC1\", \"PC2\"])\n",
    "df_pca[\"Cluster\"] = clusters\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=\"PC1\", y=\"PC2\", hue=\"Cluster\", data=df_pca, palette=\"viridis\", legend=\"full\")\n",
    "plt.title(\"K-Means Clustering Visualization (PCA)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vKqQGeukBiE5",
    "outputId": "d70d6ffe-9807-4890-fb5d-9014d1228858"
   },
   "outputs": [],
   "source": [
    "#2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"mldatasetfinal numbers.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Assuming your dataset has features and no class labels\n",
    "features = data.drop(columns=[\"Price\"])  # Adjust the column name if needed\n",
    "\n",
    "# Handle missing values by imputing with mean\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "features_imputed = imputer.fit_transform(features)\n",
    "\n",
    "# Calculate the average Euclidean distance for a range of k values\n",
    "k_values = range(1, 32)\n",
    "distortions = []\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(features_imputed)\n",
    "    distortions.append(kmeans.inertia_)  # Inertia is the average squared distance from each point to its assigned center\n",
    "\n",
    "# Plot the elbow method\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, distortions, marker='o')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Average Euclidean Distance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 768
    },
    "id": "5EmFLxZNCobp",
    "outputId": "c89d25a2-4c7f-4908-c76d-9947e704fd93"
   },
   "outputs": [],
   "source": [
    "#3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"mldatasetfinal numbers.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Assuming your dataset has features and no class labels\n",
    "features = data.drop(columns=[\"Price\"])  # Adjust the column name if needed\n",
    "\n",
    "# Handle missing values by imputing with mean\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "features_imputed = imputer.fit_transform(features)\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "agg_cluster = AgglomerativeClustering(n_clusters=None, distance_threshold=0, linkage='ward')\n",
    "clusters = agg_cluster.fit_predict(features_imputed)\n",
    "\n",
    "# Create linkage matrix for dendrogram\n",
    "linkage_matrix = linkage(features_imputed, method='ward')\n",
    "\n",
    "# Plot the dendrogram\n",
    "plt.figure(figsize=(15, 8))\n",
    "dendrogram(linkage_matrix, truncate_mode='level', p=3, leaf_rotation=45, leaf_font_size=12, show_contracted=True)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Sample Index or (Cluster Size)')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "id": "hef187p-Coly",
    "outputId": "b3cadfc7-8e04-4c8f-e345-7c4119508e93"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[2023-11-28 03:51:22] Features: 1/9 -- score: 0.5511450381679389\n",
      "[2023-11-28 03:51:50] Features: 2/9 -- score: 0.5748091603053436\n",
      "[2023-11-28 03:52:11] Features: 3/9 -- score: 0.6259541984732824\n",
      "[2023-11-28 03:52:32] Features: 4/9 -- score: 0.6553435114503816\n",
      "[2023-11-28 03:52:49] Features: 5/9 -- score: 0.666793893129771\n",
      "[2023-11-28 03:53:03] Features: 6/9 -- score: 0.6801526717557251\n",
      "[2023-11-28 03:53:14] Features: 7/9 -- score: 0.6805343511450382\n",
      "[2023-11-28 03:53:23] Features: 8/9 -- score: 0.6729007633587786\n",
      "[2023-11-28 03:53:28] Features: 9/9 -- score: 0.668702290076336"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c920263355e4>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Plot the performance of feature subsets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_sfs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_metric_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'std_dev'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Print the selected feature indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_sfs' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "id": "Na2NOWOAEGXl",
    "outputId": "7b6be6e1-d337-4d65-a10d-a8b1ac82a3b5"
   },
   "outputs": [],
   "source": [
    "#4\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the datamldatasetfinal numbers.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Assuming your dataset has features and class labels\n",
    "X = data.drop(columns=[\"Price\"])  # Features\n",
    "y = data[\"Price\"]  # Target variable\n",
    "\n",
    "# Handle missing values by imputing with mean\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Use a classifier (Random Forest in this case)\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Forward Sequential Selection\n",
    "sfs = SequentialFeatureSelector(clf, k_features=\"best\", forward=True, floating=False, verbose=2, scoring='accuracy', cv=5)\n",
    "\n",
    "# Fit the Sequential Feature Selector to the training data\n",
    "sfs.fit(X_train, y_train)\n",
    "\n",
    "# Plot the performance of feature subsets\n",
    "fig = plot_sfs(sfs.get_metric_dict(), kind='std_dev')\n",
    "\n",
    "# Print the selected feature indices\n",
    "print(\"Selected feature indices:\", sfs.k_feature_idx_)\n",
    "\n",
    "# Transform the data to include only the selected features\n",
    "X_train_selected = sfs.transform(X_train)\n",
    "X_test_selected = sfs.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "6rg5gvCHDSx7",
    "outputId": "89c77e91-d3d7-48c3-871b-ddffc44836ef"
   },
   "outputs": [],
   "source": [
    "#5\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"mldatasetfinal numbers.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Assuming your dataset has features and no class labels\n",
    "features = data.drop(columns=[\"Price\"])  # Adjust the column name if needed\n",
    "\n",
    "# Handle missing values by imputing with mean\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "features_imputed = imputer.fit_transform(features)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA()\n",
    "features_pca = pca.fit_transform(features_imputed)\n",
    "\n",
    "# Calculate the cumulative explained variance ratio\n",
    "cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Find the number of components needed to capture 95% of the variance\n",
    "num_components_95_percent = np.argmax(cumulative_variance_ratio >= 0.95) + 1\n",
    "\n",
    "# Print the results\n",
    "print(\"Number of features needed to capture 95% of data variance:\", num_components_95_percent)\n",
    "\n",
    "# Plot the explained variance ratio\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), cumulative_variance_ratio, marker='o')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.title('Explained Variance Ratio vs. Number of Principal Components')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JLf3QZxfDS0O",
    "outputId": "4e4ce3b1-eb3e-4ed4-a2ce-4548646b730f"
   },
   "outputs": [],
   "source": [
    "#6\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"mldatasetfinal numbers.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Assuming your dataset has features and class labels\n",
    "X = data.drop(columns=[\"Price\"])  # Features\n",
    "y = data[\"Price\"]  # Target variable\n",
    "\n",
    "# Handle missing values by imputing with mean\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Perform PCA and transform the data\n",
    "pca = PCA(n_components=num_components_95_percent)\n",
    "X_pca = pca.fit_transform(X_imputed)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Use a classifier (Random Forest in this case)\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy using transformed dataset:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
